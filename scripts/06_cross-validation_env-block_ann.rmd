---
title: "Cross validation using environmental blocking"
subtitle: "Based on annual fire data"
author: "Martin Holdrege"
date: "`r lubridate::today()`"
params:
  test_run: TRUE
  s: '_ann_A-P'
  n_train: 5000000
  sample_group: 1
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---

The goal here is to split the data into multiple training/testing
blocks based on clustering grid-cells by climate.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,
                      message = FALSE)
```


# Dependencies


```{r, message = FALSE, warning = FALSE}
# set option so resampled datasets created here reproduces earlier runs of this code with dplyr 1.0.10
#source("scripts/04_create_biome-mask_dataframe_byNFire.R")
source("src/resample_functions.R")
source("src/modeling_functions.R")
source("src/general_functions.R")
source("src/basemaps.R")
source("src/fig_params.R")
library(blockCV) # for creating environmental blocks
library(sf)
library(knitr)
library(tidyverse)
library(stars)
```

params

```{r}
print(params)
test_run <- params$test_run # runs code on data subset, for testing
s <- params$s
n_train <- params$n_train
sample_group <- params$sample_group
```


## Read in data

Read in model object (for the formula)

```{r}
path_mod <- paste0("models/glm_binomial_models_v1", s, "_", n_train, "n_g", 
        sample_group, ".RDS")
mod1 <- readRDS(path_mod)
```

Reading in predictor vars

```{r}
df_ann1 <- read_csv('data_processed/fire-clim-veg_3yrAvg_v1.csv',
                    progress = FALSE, show_col_types = FALSE)

df_ann2 <- df_ann1 %>% 
  select(cell_num, pfgAGB, afgAGB, nfire_cwf, MAP, MAT, prcpPropSum)
```

Template (cell number) raster

```{r}
template <- rast("data_processed/data_publication/cell_nums.tif")
```



# functions

```{r}
# summarizing across columns of interest
summarize_pred_response <- function(df) {
  out <- df %>% 
    mutate(MAT = MAT - 273.15,
           # convert from probability to percent
         across(matches("cwf_prop"),
                        .fns = function(x) x*100)) %>% 
    summarise(across(c(MAT, MAP, prcpPropSum, afgAGB, pfgAGB, matches("cwf_prop")),
                  .fns = weighted.mean))
  out
}
```


# Determine Blocks


Combine climate data into a single raster for use in blockCV::envBlock(),
and create an sf object of fake datapoints so that function will work. 

```{r}
# average climate conditions across all the years 
df_clim_avg <- df_ann1 %>% 
  select(cell_num, MAT, MAP, prcpPropSum) %>% 
  group_by(cell_num) %>% 
  summarise(across(everything(), mean))

r_clim2 <- fill_raster(df_clim_avg, template)

n <- ncell(r_clim2)
cell_coords <- xyFromCell(template, 1:n) %>% 
  as.data.frame() %>% 
  mutate(
    cell_num = as.numeric(values(template))) %>% 
  filter(!is.na(cell_num)) %>% 
  # sf object of the center of sagebrush biome grid points
  st_as_sf(coords = c('x', 'y'), crs = crs(template))


k <- 5
fold_name <- paste0('fold', 1:k)

set.seed(124) # envBlock uses kmeans which is affected by the seed

# environmental blocks
eb1 <- blockCV::cv_cluster(x = cell_coords, # coords of center of eath grid cell
                           r = r_clim2,
                           k = k,
                           scale = TRUE,
                           raster_cluster = FALSE)


# checking that size of all test datasets sums to the 
# number of grid cells in the original dataset
cell_nums <- unique(df_clim_avg$cell_num)
stopifnot(length(unique(cell_nums)) 
          == sum(eb1$records$train[1] + eb1$records$test[1]))
```

# Prepare Data


```{r}
pred_vars <- c("afgAGB", "pfgAGB", "MAT", "MAP", "prcpPropSum")
clim_vars <- c("MAT", "MAP", "prcpPropSum")
names(pred_vars) <- pred_vars
response_vars <- c("cwf_prop", "cwf_prop_pred")

```


## Split into train/test

```{r}
fold_endices <- cell_coords
fold_endices$geometry <- NULL
fold_endices$fold <- eb1$folds_ids
fold_endices <- as_tibble(fold_endices) %>% 
  # renumbering so number (and colors) will
  # (more or less) match with the first submission
  # to fire ecology (for easier interpretation)
  mutate(fold = c(1, 2, 4, 3, 5)[fold]) %>% 
  arrange(fold, cell_num)
test_cell_nums <- fold_endices %>% 
  mutate(fold = paste0('fold', fold)) %>% 
  group_by(fold) %>% 
  group_split()
names(test_cell_nums) <- fold_name
# data for a given fold
dfs_test1 <- map(test_cell_nums, function(x) {
  df_ann2[df_ann2$cell_num %in% x$cell_num, ] 
})

# data for the other folds
dfs_train1 <- map(test_cell_nums, function(x) {
  df_ann2[!df_ann2$cell_num %in% x$cell_num, ] 
})

```

```{r}
if (test_run) {
  n <- 1000
  dfs_test1 <- map(dfs_test1, ungroup) %>% 
    map(sample_n, size = n)
  dfs_train1 <- map(dfs_train1, ungroup) %>% 
    map(sample_n, size = n)
  df_ann3 <- sample_n(ungroup(df_ann2), size = n*5)
} else {
  df_ann3 <- df_ann2
}
```


# Describe folds



```{r}
cols_fold <- RColorBrewer::brewer.pal(k, 'RdYlBu')
r_folds1 <- fill_raster(fold_endices, template)

writeRaster(r_folds1, 'data_processed/cv_folds_v2.tif', overwrite = TRUE)

# the base and bbox objects come from src/basemaps.R
g <- ggplot() +
  geom_stars(data = st_as_stars(r_folds1), aes(fill = factor(fold, levels = 1:k))) +
  basemap_g() +
  scale_fill_manual(values = cols_fold,
                     na.value ='white',
                    name = 'Fold',
                    na.translate = FALSE) 
g
ggsave("figures/cv/folds_map_v1.png", plot = g,  width = 4, height = 4,
       dpi = 600)

png("figures/cv/folds_map_v1.png",width = 5, height = 4, units = 'in',
       res = 600)
g
dev.off()

```


STOP--continue working here

# Fit models

All models have the same formula. But are fit to data where one
block of data is held out. 

```{r}
form1 <- as.formula(mod1$formula)

mods_train1 <- map(dfs_train1, function(df) {
  glm(form1, family = 'binomial', data = df)
})


# predictions on the test data set
dfs_test2 <- map2(mods_train1, dfs_test1, function(mod, df) {
  df$cwf_prop_pred <- predict(mod, newdata = df, type = 'response')
  df
})

```


## PDP

These are true PDP plots--an earlier version of this code created
'pseudo' pdp plots where I was predicting across the range of a given predictor
variable, with other variables held at their mean. 


```{r}
lookup_var <- var2lab(x = NULL, units_md = TRUE)

mods_train2 <- c(mods_train1, list('biome-wide' = mod1))

letter_df <- tibble(
  letter = fig_letters[1:length(lookup_var)],
  pred_var_name = factor(lookup_var),
  value = -Inf,
  yhat = Inf
)

```

```{r create_pdp_df, cache = FALSE, eval= !test_run}
df_pdp_by_fold1 <- map_dfr(mods_train2, create_pdp_df, 
                   mod_vars = names(lookup_var),
                   .id = 'fold')
```


```{r fig.height = 3.5, fig.width = 6.8, eval = !test_run}
df_pdp_by_fold2 <- df_pdp_by_fold1 %>% 
  mutate(pred_var_name = var2lab(pred_var, units_md = TRUE))

g2 <- ggplot(df_pdp_by_fold2, aes(value, yhat*100)) +
  geom_line(aes(color = fold)) +
  geom_text(data = letter_df, aes(label = letter),
            hjust = -0.8,
            vjust = 1) +
  facet_wrap(~pred_var_name, scales = 'free', strip.position = "bottom") +
  theme(strip.text = ggtext::element_markdown(),
      strip.placement = "outside",
      axis.title.x = element_blank(),
      strip.background = element_blank()) +
    expand_limits(y = 3) +
  labs(y = lab_fireProbPerc)+
  scale_color_manual(values = c(cols_fold, 'biome-wide' = 'black'), 
                     name = "Fold left out")

jpeg(paste0("figures/pdp/cv_pdp_v1", s, ".jpeg"), units = "in", res = 600,
     width = 6.8, height = 4)
  g2
dev.off()
g2
```

## Summary stats

Summarizing fire occurrence by fold. Note -here using the raw (not balanced)
data. And predictions are made by the main model. 

```{r}
df_byNFire2_fold <- df_byNFire2 %>% 
  left_join(df_test_cell_nums, by = 'cell_num')

df_byNFire2_fold <- df_byNFire2_fold %>% 
  split(., .$fold) %>%
  # cross validation predictions
  # for each fold creat prediction from model not fit to data from that fold
  map2(mods_train1, function(df, mod) {
    df$cwf_prop_pred_cv <- predict(mod, newdata = df,
                                   type = 'response')
    df
  }) %>% 
  bind_rows(.id = "fold")

# prediction from the main model
df_byNFire2_fold$cwf_prop_pred_main <- predict(mod1, newdata = df_byNFire2_fold,
                                 type = 'response')

digits <- c(NA, 1, 0, 3, 1, 1, 2, 2, 2)

df_byNFire2_fold %>% 
  group_by(fold) %>% 
  summarize_pred_response() %>% 
  kable(caption = "Mean pred vars and observed/predicted fire probability by fold",
        digits = digits) 

df_byNFire2_fold %>% 
  ungroup() %>% 
  summarize_pred_response() %>% 
  kable(caption = "Mean pred vars and observed/predicted fire probability across biome",
        digits = digits[-1]) 
```

Descriptive stats for each fold, including the out of bag predictions
Note unlike in the section above 'describe folds' these descriptions here
are of the balanced (resampled) data

```{r}
df_test2 <- bind_rows(dfs_test2, .id = 'fold') %>% 
  # these are the cross validation predictions
  rename(cwf_prop_pred_cv = cwf_prop_pred)

df_test2$cwf_prop_pred_main <- predict(mods_train2$`biome-wide`, 
                                          newdata = df_test2,
                                          type = "response")

df_test2  %>% 
  group_by(fold) %>% 
  summarize_pred_response() %>%  
  kable(caption = "Summary of variables by fold (balanced data)",
        digits = digits)

df_test2  %>% 
  ungroup() %>% 
  #select(-cwf_prop_pred_cv) %>% 
  summarize_pred_response() %>%  
  kable(caption = "Summary of variables (biome-wide) (balanced data)",
        digits = digits[-1])
```

## size

Note here the number of observations per fold means the number of rows
of data (not # of pixels). Some pixels have multiple observations (i.e.,
when a fire occurred)

```{r}
# number of grid cells in the fold (original, raw data)
fold_size <- df_byNFire2_fold %>% 
  group_by(fold) %>% 
  summarize(fold_size_raw = n()) 

# number of grid cells in the fold (resampled data)
fold_size_resample <- dfs_test1 %>% 
  bind_rows(.id = 'fold') %>% 
  group_by(fold) %>% 
  summarize(fold_size_resampled = n())

fold_size %>% 
  left_join(fold_size_resample, by = 'fold') %>% 
  select(fold, everything()) %>% 
  kable(caption = "Number of observations per fold")

```


## Observed vs predicted

### Deciles

```{r fig.width = 5, fig.height = 3.5 }

pred_test_deciles1 <- map(dfs_test2, predvars2deciles, pred_vars = pred_vars,
                 response_vars = response_vars)

map2(pred_test_deciles1, names(pred_test_deciles1), function(df, fold) {
  decile_dotplot_pq(df = df) +
    labs(subtitle = paste0("Predictions on ", fold, 
                           " (model fit to other folds)")) +
    coord_cartesian(ylim = c(0, 3.5))
})
```

### original data

Quantile plots made based on the observed & predicted values from
the original data (not resampled)

```{r fig.width = 5, fig.height = 3.5 }

df_byNFire2_fold <- df_byNFire2_fold %>% 
  # renaming needed for functions below to work
  rename(cwf_prop_pred = cwf_prop_pred_main)

pred_orig_deciles1 <- df_byNFire2_fold %>% 
  split(., .$fold) %>% 
  map(predvars2deciles, pred_vars = pred_vars,
      response_vars = response_vars)


map2(pred_orig_deciles1 , names(pred_orig_deciles1), function(df, fold) {
  decile_dotplot_pq(df = df) +
    labs(subtitle = paste0("Predictions on ", fold, 
                           " (by main model)"),
         caption = "Observed and predicted values calculated from the raw
         (not resampled) data")
})
```

average predicted/observed for fold 5 (checking
because some model bias in that fold)

```{r}
pred_orig_deciles1$fold5 %>% 
  group_by(name) %>% 
  summarise(observed = mean(cwf_prop)*100,
         predicted = mean(cwf_prop_pred)*100)
```


### deciles filtered

Predictions are made by models fit to the other folds

```{r}
pred_test_deciles_filt <- map(dfs_test2, predvars2deciles, pred_vars = pred_vars,
                 response_vars = response_vars,
                 filter_var = TRUE)
```

```{r fig.width = 7, fig.height = 6 }

g_filt <- map2(pred_test_deciles_filt, names(pred_test_deciles_filt), function(df, fold) {
  g <- decile_dotplot_filtered_pq2(df = df) +
    patchwork::plot_annotation(subtitle = paste0("Predictions on ", fold, 
                           " (model fit to other folds)"))
  
  jpeg(paste0("figures/quantile_plots/cv_quantile_plots_filt_", fold, s, 
              ".jpeg"), units = "in", res = 600,width = 7, height = 6 )
    print(g) 
  dev.off()
  g
})

g_filt
```

### deciles filtered (veg)

Predictions for each fold made by the main model (i.e. this part isn't actually
cross validation)

```{r fig.height = 10, fig.width = 5}

pred_test_deciles_filt_main <- df_test2 %>%
  rename(cwf_prop_pred = cwf_prop_pred_main) %>% 
  split(., .$fold) %>% 
  map(predvars2deciles, pred_vars = pred_vars,
                 response_vars = response_vars,
                 filter_var = TRUE,
                 filter_vars = pred_vars)

figs <- map(pred_test_deciles_filt_main, decile_dotplot_filtered_pq)

map2(figs, names(figs), function(g, fold) {
  g +
    labs(subtitle = paste('Balanced data from', fold, 
                          '(predictions by main model)'))
})
```

### deciles filtered (clim)

```{r fig.height = 10, fig.width = 7}
figs2 <- map(pred_test_deciles_filt_main, decile_dotplot_filtered_pq,
    xvars = clim_vars)

map2(figs2, names(figs2), function(g, fold) {
  g +
    labs(subtitle = paste('Balanced data from', fold, 
                          '(predictions by main model)'),
         caption = 'y limits constrained') +
    coord_cartesian(ylim = c(0, 4))
})
```



# Model selection 

Fitting separate models to each fold, and in each case going through model
selection (i.e., seeing which transformations are best). Doing this to
figure out how the relationships are different between folds. 

Creating formulas where each variable on its own is transformed numerous ways, all other variables are left alone,
that repeated for each variable. So have models with 1 variable transformed,
2 transformed, etc. 

see documentation for glms_iterate_transforms, in the modelling_functions.R
script

## Iteration

```{r iterate_transform, warning = FALSE}
set.seed(1234)

# parentheses around interaction
# term are included so that glms_iterate_transforms doesn't transform
# the interaction term. 

# some older model objects don't have interactions included
# so doing this so the code doesn't crash
pred_vars_inter <- if(!is.null(mod1$pred_vars_inter)) {
  mod1$pred_vars_inter
} else {
  message('actual model interactions not provided')
  c(pred_vars, 'afgAGB:MAP' = "(afgAGB:MAP)",
    "afgAGB:prcpPropSum" = "(afgAGB:prcpPropSum)")
}
# pred_vars_inter <- pred_vars

best_formulas1 <- map(dfs_test1, function(df) {
  out <- glms_iterate_transforms(
    preds = pred_vars_inter, 
    df = df,
    response_var = "cwf_prop", 
    delta_aic = 10,
    weights_col = "numYrs")

  out
})

```

## formulas

formulas of the 'best' model for each fold

```{r}
best_formulas2 <- map(best_formulas1, function(x) x$final_formula)
best_formulas2
```

fit best models. The function above does not save the model objects
so here re-creating the best models

```{r}
best_mods1 <- map2(best_formulas2, dfs_test1, function(form, df) {
  glm(as.formula(form), data = df , family = 'binomial', weights = numYrs)
})
```

## PDP

Creating partial dependence plots of the best models fit to each fold

```{r pdp_best}

df_pdp_best_mod_fold1 <- map_dfr(best_mods1, create_pdp_df, 
                   mod_vars = names(lookup_var),
                   .id = 'fold')

```

```{r fig.height = 3.5, fig.width = 6.8}
df_pdp_best_mod_fold1 <- df_pdp_best_mod_fold1 %>% 
  mutate(pred_var_name = var2lab(pred_var, units_md = TRUE))

g2 <- ggplot(df_pdp_best_mod_fold1, aes(value, yhat*100)) +
  geom_line(aes(color = fold)) +
  facet_wrap(~pred_var_name, scales = 'free', strip.position = "bottom") +
  theme(strip.text = ggtext::element_markdown(),
      strip.placement = "outside",
      axis.title.x = element_blank(),
      strip.background = element_blank()) +
    expand_limits(y = 3) +
  labs(y = lab_fireProbPerc)+
  scale_color_manual(values = cols_fold, 
                     name = "Fold model fit to")

g2
```

# session info

Hash of current commit (i.e. to ID the version of the code used)

```{r}
system("git rev-parse HEAD", intern=TRUE)
```

Packages etc.

```{r}
sessionInfo()

```











